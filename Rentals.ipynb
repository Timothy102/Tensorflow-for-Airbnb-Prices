{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting New York City rental prizes from Airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're pulling the data straight off the kaggle website. We could also download the archive zip file, but we're going to use the csv file directly. Note: by using the -o flag we're specifying the output filename. Mine is airbnb.csv. You can of course use any name you'd like, just note to pass it as to pandas' read_csv function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -o airbnb.csv https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='airbnb.csv'\n",
    "df=pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has _ columns and _rows. Pandas' dataframe object has a lot of built in options for managing and analysing data, as you'll see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has a great plotting API. Let's use it to plot the correlation matrix. Change the figsize parameters for the size of the canvas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(method='kendall')\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(corr, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll remove the duplicates and deal with illegal values in the column 'reviews_per_month'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['reviews_per_month']=df['reviews_per_month'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the types of columns we're dealing with here. \n",
    "\n",
    "Dropping unnecessary columns will make it easier to train our model.\n",
    "When I say unnecessary, I mean data that has low correlation with the model's prediction. \n",
    "Let's take a look at the types after dropping columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['id','host_id','name','host_name','last_review','calculated_host_listings_count']\n",
    "df=df.drop(columns,axis=1)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the countplot we'll recieve the visual information about which neighbourhood group is the most popular for renting via AirBnb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['neighbourhood'], palette=\"plasma\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.title('Neighbourhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dims=(12,8)\n",
    "plt.figure(figsize=plot_dims)\n",
    "sns.scatterplot(df.longitude,df.latitude,hue=df.neighbourhood_group)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll one-hot encode certain columns for training. Computers deal with numbers, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['neighbourhood_group']=pd.factorize(df.neighbourhood_group)[0]\n",
    "df['neighbourhood']=pd.factorize(df.neighbourhood)[0]\n",
    "df['room_type']=pd.factorize(df.room_type)[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the availability column so that its values don't return out of the box loss results while training.\n",
    "# The column's mean is 0 and standard deviation is 1 \n",
    "\n",
    "availabillity=df['availability_365']\n",
    "availabillity=(availabillity-availabillity.mean())/availabillity.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I created separate datasets for columns and labels. Note that the X dataset does not remove the price label. Remember we are trying to predict the price based on the training data. \n",
    "\n",
    "Data shapes don't match, so I've decided to truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['price']\n",
    "X=df.copy()\n",
    "X=X.drop(X['price'])\n",
    "\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=Y.truncate(after=48220,axis=0)\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are the feature crosses.\n",
    "The point is to merge the two columns, so that its values are representative of the data. Our goal here is to feature cross longitude and landitute, which is one of the oldest tricks in the book. If we put merely the two columns as values to the model, it will assume those values are progressively related to the output. \n",
    "\n",
    "Instead, we'll be using a feature cross, meaning we will split the longitude*langitude map into a grid. \n",
    "Quite a delicate little problem. Lucky for us, Tensorflow makes it easy. \n",
    "\n",
    "\n",
    "I'm making a grid of equally spreaded grids by iterating from the minimum to the maximum value with an iteration of (max-min)/100.\n",
    "\n",
    "I'm using a 100x100 grid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_long=df['longitude'].max()\n",
    "min_long=df['longitude'].min()\n",
    "\n",
    "diff=max_long-min_long\n",
    "diff/=100\n",
    "\n",
    "long_boundaries=[]\n",
    "for i in np.arange(min_long, max_long, diff):\n",
    "    long_boundaries.append(min_long+i*diff)\n",
    "\n",
    "    \n",
    "max_lat=df['latitude'].max()\n",
    "min_lat=df['latitude'].min()\n",
    "\n",
    "d=max_lat-min_lat\n",
    "d/=100\n",
    "\n",
    "lat_boundaries=[]\n",
    "for i in np.arange(min_lat, max_lat, d):\n",
    "    lat_boundaries.append(min_long+i*d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, what we're doing here, is defining a bucketized column with boundaries defined earlier and creating a DenseFeatures layer, which will be passed to the Sequential API later. \n",
    "\n",
    "\n",
    "If you're not familiar with the Tensorflow syntax, do check the docs.\n",
    "https://www.tensorflow.org/api_docs/python/tf/feature_column/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_marked=tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('longitude'), boundaries=long_boundaries\n",
    ")\n",
    "\n",
    "lat_marked=tf.feature_column.bucketized_column(\n",
    "   tf.feature_column.numeric_column('latitude'),boundaries=lat_boundaries\n",
    ")\n",
    "\n",
    "\n",
    "crossed_feature=tf.feature_column.crossed_column([long_marked,lat_marked],hash_bucket_size=100)\n",
    "feature_layer=tf.keras.layers.DenseFeatures(tf.feature_column.indicator_column(crossed_feature))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we'll finally prepare the data for training using the sklearn's train_test_split and the StandardScaler function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, sklearn makes it super easy to define this hard architectures.\n",
    "First, linear regression, which shouldn't work well because this isn't a regression task. \n",
    "\n",
    "Then, the notorious support vector machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "linreg=LinearRegression()\n",
    "linreg.fit(x_train,y_train)\n",
    "y_pred=linreg.predict(x_test)\n",
    "r2_score=r2_score(y_test,y_pred)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc=SVC(kernel='linear')\n",
    "svc.fit(x_train,y_train)\n",
    "r2_score=r2_score(y_test,y_pred)\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the creation of the keras Sequential model. \n",
    "\n",
    "We' re compiling the model using the Adam optimizer, MSE loss and two metrics. Keep track of these while the model trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout,DenseFeatures\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "model = tf.keras.Sequential([\n",
    "    DenseFeatures(feature_layer)\n",
    "    Dense(128,activation='relu'),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "rmse=tf.keras.metrics.RootMeanSquaredError()\n",
    "model.compile(optimizer=opt,loss='mean_squared_error',metrics=['mae',rmse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we are using two callbacks\n",
    "\n",
    "EarlyStopping, which is self explanatory, but check the docs \n",
    "   - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    \n",
    "Reduce learning rate on plateau.\n",
    "  - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=2,factor=0.2)\n",
    "early_stopper=tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "callbacks=[lr_reducer,early_stopper]\n",
    "\n",
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test), callbacks=callbacks,epochs=50,batch_size=64,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df=pd.DataFrame(history.history)\n",
    "history_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df.plot(x='loss',y='mae')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Mae')\n",
    "plt.title(\"Model performance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
